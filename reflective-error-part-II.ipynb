{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287bd735-9427-44a3-afea-f54fa5b7ea7d",
   "metadata": {},
   "source": [
    "# Reflective Error - Part II\n",
    "\n",
    "## A Real World Example in Hydrology\n",
    "This second notebook reproduces the second set of experiments, the application of the Reflective Error functions within a hydrological context, in the Environmental Data Science paper \"Reflective Error: A Metric for s Predictive Performance at Extreme Events\" and is designed to provide a step by step guide on the application of both the metric and the associated loss function.  As explained in the paper, the metric and loss function are designed to give a generalisable quantification of model error with respect to the underlying probability distribution that best fits the data.  This notebook follows on from the [Part I notebook](./reflective-error-part-I.ipynb).\n",
    "\n",
    "To get started, we'll import all of the prerequisite libraries, including numpy, pandas, scipy, xarray, and pytorch, and we'll also set some global parameters for graph presentation, random number seeds for reproducability, and torch device configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a40ad9ec-53c4-4e1b-943a-363ede43e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cdsapi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtk\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "fontlibrary = str('font.' + 'serif')\n",
    "plt.rcParams[fontlibrary] = ['Times New Roman']\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a422f2-49db-46c5-9cd9-868b283ecea2",
   "metadata": {},
   "source": [
    "As per the [Part I notebook](./reflective-error-part-I.ipynb), the metrics that we'll be using in this notebook are Root Mean Squared Error (RMSE), R$^2$, and Reflective Error (RE).  For RE, $\\Psi_{\\phi}(y_i)$ is the weighting to be applied to the error at any given $y$ based on the underlying, probability distribution corresponding to values $y$ within the local subdomain, $\\phi$, of the target domain, $Y$.  Note that for the stationary form of RE, $\\phi = Y$.  The scaling factor $\\kappa = \\underset{y}{\\max} \\big(U(y)\\big)$ and $\\alpha$ and $\\beta$ are parameters set to $1$ for the metric and hyperparameters to be tuned by the user for the reflective loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced57745-34e4-48fe-ac43-f69e5fb53fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_o, y_p):\n",
    "    '''\n",
    "    Function to evalate the root mean squared error\n",
    "    between a set of observations and predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_o : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of observations, y\n",
    "    y_p : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of predictions, y'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse : Float\n",
    "        Root Mean Squared Error\n",
    "    '''\n",
    "    total = (((y_o - y_p)**2)/len(y_o))\n",
    "    rmse = sum(total)**0.5\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def R2(y_o, y_p):\n",
    "    '''\n",
    "    Function to evalate the r2 value between\n",
    "    a set of observations and predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_o : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of observations, y\n",
    "    y_p : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of predictions, y'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    r2 : Float \n",
    "        R2\n",
    "    '''\n",
    "    cache1 = ((y_o - y_p)**2)\n",
    "    mu = np.mean(y_o)\n",
    "    cache2 = ((y_o - mu)**2)\n",
    "    r2 = 1 - (sum(cache1))/(sum(cache2))\n",
    "    return r2\n",
    "\n",
    "def PSI(u_of_y, kappa, alpha=1, beta=1):\n",
    "    '''\n",
    "    Function to evalate the weighting to be applied\n",
    "    elementwise to error terms with \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u_of_y : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Probability of y, elementwise, according to\n",
    "        the distribution fitted to training data\n",
    "    alpha : Float\n",
    "        Reflective scaling hyperparameter.\n",
    "    beta : Float\n",
    "        Reflective shift hyperparameter.\n",
    "    kappa : Float\n",
    "        Global maximum of u_of_y (recommend using calculus to find).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    psi : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Reflective weighting to be applied in the loss\n",
    "        function during network training.\n",
    "    '''\n",
    "    psi  = -1 * alpha * (u_of_y/kappa) + beta\n",
    "    return psi\n",
    "\n",
    "\n",
    "def RE(y_o, y_p, psi):\n",
    "    '''\n",
    "    Function to evalate the reflective error\n",
    "    between a set of observations and predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_o : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of observations, y\n",
    "    y_p : Float, Numpy Array, or Pandas DataFrame Column\n",
    "        Set of predictions, y'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    r_rmse : Float\n",
    "        Reflective Root Mean Squared Error\n",
    "    '''\n",
    "    cache1 = ((y_o - y_p)**2)\n",
    "    re = ((sum(cache1 * psi))/(sum(cache1)))**(0.5)\n",
    "    return re\n",
    "\n",
    "\n",
    "def RELossFunc(prediction, target, psi):\n",
    "    '''\n",
    "    Function to calculate unaggregated loss that can be processed\n",
    "    using the machine learning library specified by the user\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : Float, array, or Tensor\n",
    "        Machine learning algorithm output.\n",
    "    target :  Float, array, or Tensor\n",
    "        Observation to compare the prediction against.\n",
    "    gamma : Float, array, or Tensor\n",
    "        Reflective weighting penalty applied elementwise to the\n",
    "        prediction and target pairs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    interrim_loss : Float, array, or Tensor\n",
    "        Unaggregated loss between prediction(s) and target(s).\n",
    "    '''\n",
    "    interrim_loss = (((prediction - target)**2) * psi)\n",
    "    return interrim_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28ef9-e532-4d85-9f0f-232c427ed9e5",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "There are two sources from which we need to obtain data: the first is the streamflow data itself, which is taken from the Centre for Ecology & Hydrology's [National River Flow Archive](https://nrfa.ceh.ac.uk/); and the second is ERA5 meteorological data, which can be downloaded using the European Centre for Medium-Range Weather Forecasts' Copernicus Data Store.\n",
    "\n",
    "The following two links provide direct access to the csv files containing the streamflow records for the two rivers used in this study: [Avon at Bathford](https://nrfa.ceh.ac.uk/data/station/meanflow/53018) & [Exe at Pixton](https://nrfa.ceh.ac.uk/data/station/meanflow/45009).  The NRFA also has an API that can be queried for additional data, with instructions on how to use said API available [here](https://nrfa.ceh.ac.uk/content/nrfa-api).\n",
    "\n",
    "In order to download data through the CDS API, users will need to set up a personal access token and download the requisite python library.  Full instructions can be found [here](https://cds.climate.copernicus.eu/how-to-api).  The following script will download the required data once setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69c26e61-1ec4-44c3-8eb6-73a10aeb2791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query(filename, product, variables, gridsquare, years, months,\n",
    "                   days, times, pressure_levels='n/a'):\n",
    "    '''\n",
    "    Creates a standardised dataquery in dictionary format that can be passed\n",
    "    to initialise the era5 class and subsequent download from the copernicus\n",
    "    data store.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : String\n",
    "        Filename stem to which the data will be saved.\n",
    "    product : String\n",
    "        ERA5 data product that is being accessed.  Refer to the CDS API for\n",
    "        guidance on product names if further information is required.\n",
    "    variables : String, List of strings\n",
    "        Meteorologial variables to extract from the data product.\n",
    "    gridsquare : String, List of strings\n",
    "        Lat/Lon gridsquare over which to extract meteorological data.\n",
    "    years : Integer, String, List of integers or strings\n",
    "        Years at which to extract meteorological data.\n",
    "    months : Integer, String, List of integers or strings\n",
    "        Months at which to extract meteorological data.\n",
    "    days : Integer, String, List of integers or strings\n",
    "        Days at which to extract meteorological data.\n",
    "    times : Integer, String, List of integers or strings\n",
    "        Hours at which to extract meteorological data.\n",
    "    pressure_levels : Integer, String, List of integers or strings, optional\n",
    "        Pressure levels from which to extract varibles. The default is 'n/a'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query : Dictionary\n",
    "        Standardised data query format.\n",
    "    '''\n",
    "    query = {'file_stem':filename, 'product':product, 'variables':variables,\n",
    "               'area':gridsquare, 'years':years, 'months':months, 'days':days,\n",
    "               'times':times, 'pressure_levels':pressure_levels\n",
    "               }\n",
    "    return query\n",
    "\n",
    "class era5():\n",
    "    def __init__(self, query):\n",
    "        '''\n",
    "        Initialises the era5 data query class for accessing and downloading\n",
    "        data via the CDS API.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : Dictionary\n",
    "            Standardised data query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "        '''\n",
    "        self.c = cdsapi.Client()\n",
    "        self.query = query\n",
    "        self.pressure_set = query['pressure_levels']\n",
    "\n",
    "    def download(self):\n",
    "        '''\n",
    "        Initiates the download from the CDS, outputting data to the \n",
    "        target filename.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "        '''\n",
    "        if self.pressure_set=='n/a':\n",
    "            filename = str(self.query['file_stem']) + '.nc'\n",
    "            self.c.retrieve(self.query['product'],\n",
    "                            {\"product_type\":   [\"reanalysis\"],\n",
    "                            \"data_format\":         [\"netcdf\"],\n",
    "                            \"variable\":       self.query['variables'],\n",
    "                            \"area\":           self.query['area'],\n",
    "                            \"year\":           self.query['years'],\n",
    "                            \"month\":          self.query['months'],\n",
    "                            \"day\":            self.query['days'],\n",
    "                            \"time\":           self.query['times']},\n",
    "                            filename)\n",
    "        else:\n",
    "            for p in self.pressure_set:\n",
    "                filename = str(self.query['file_stem']) + str(p) + 'hPa.nc'\n",
    "                self.c.retrieve(self.query['product'],\n",
    "                                {\"product_type\":   \"reanalysis\",\n",
    "                                \"data_format\":         \"netcdf\",\n",
    "                                \"pressure_level\": [p],\n",
    "                                \"variable\":       self.query['variables'],\n",
    "                                \"area\":           self.query['area'],\n",
    "                                \"year\":           self.query['years'],\n",
    "                                \"month\":          self.query['months'],\n",
    "                                \"day\":            self.query['days'],\n",
    "                                \"time\":           self.query['times']},\n",
    "                                filename)\n",
    "\n",
    "area = ['60.00/-8.00/48.00/4.00']\n",
    "yyyy = [str(y) for y in range(1979,2022,1)]\n",
    "mm = [str(m) for m in range(1,13,1)]\n",
    "dd = [str(d) for d in range(1,32,1)]\n",
    "hh = [str(t).zfill(2) + ':00' for t in range(0, 24, 1)]\n",
    "met = ['total_precipitation','temperature','u_component_of_wind',\n",
    "       'v_component_of_wind','relative_humidity',\n",
    "       'volumetric_soil_water_layer_1','volumetric_soil_water_layer_2',\n",
    "       'volumetric_soil_water_layer_3','volumetric_soil_water_layer_4',]\n",
    "\n",
    "for yy in yyyy:\n",
    "    filename = 'Rainfall_' + str(yy)\n",
    "    rain_query = er.query(filename, 'reanalysis-era5-single-levels', met[0],\n",
    "                          area, yy, mm, dd, hh)\n",
    "    rain_data = er.era5(rain_query).download()\n",
    "    er.aggregate_mean(str(rain_query['file_stem']) + '.nc',\n",
    "                      str(rain_query['file_stem']) + '_aggregated.nc')\n",
    "full_rain_data = xr.open_mfdataset('Rainfall_*_aggregated.nc', concat_dim='time')\n",
    "full_rain_data.to_netcdf(path='Rainfall.nc')\n",
    "pressure_query = er.query('Pressure','reanalysis-era5-pressure-levels',\n",
    "                          met[1:5], area, yyyy, mm, dd, '12:00', ['1000'])\n",
    "pressure_data = er.era5(pressure_query).download()\n",
    "soil_query = er.query('Soil_Moisture','reanalysis-era5-land', met[5:],\n",
    "                      area, yyyy, mm, dd, '12:00')\n",
    "soil_data = er.era5(soil_query).download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3377cb-e250-4873-bd27-db109e281d01",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "With the data downloaded, we will now preprocess it by extracting meteorological variables, including antecedent proxies, at the centroid of the catchment, with catchment boundaries taken from the NRFA database as shape files along with the gauged daily flow data for the pair of target catchments; for convenience, metadata for these catchments is stored in the file ['Catchment_Database.csv'](./Catchment_Database.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a350d-d396-412a-acad-d0dd992d4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(df):\n",
    "    '''\n",
    "    Dictionary based renaming of columns within a dataframe from ECMWF\n",
    "    nomenclature to plain language.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Original dataframe of variables pulled from ERA5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : Pandas dataframe\n",
    "        Dataframe with renamed columns for human readability.\n",
    "    '''\n",
    "    df = df.rename(columns={'time':'Date','tp':'Rain',\n",
    "                            't':'Temperature','r':'Humidity',\n",
    "                            'u':'U Windspeed','v':'V Windspeed',\n",
    "                            'swvl1':'Soil Moisture 1',\n",
    "                            'swvl2':'Soil Moisture 2',\n",
    "                            'swvl3':'Soil Moisture 3',\n",
    "                            'swvl4':'Soil Moisture 4'})\n",
    "    return df\n",
    "\n",
    "def weather_shift(df, variable, ndays, past=True):\n",
    "    '''\n",
    "    Creates additional columns by copying meteorological variables and\n",
    "    shifting them back or forward in time for a given number of steps,\n",
    "    creating rows of data with the required antecedent record that can be\n",
    "    passed directly into a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xf : Pandas dataframe\n",
    "        Dataframe of variables to be shifted in time.\n",
    "    variable : String\n",
    "        Name of the feature column to be shifted.\n",
    "    ndays : Integer\n",
    "        Number of days, or timesteps, to apply the shift.\n",
    "    past : Boolean, optional\n",
    "        Applying either a rearward or forward looking shift.\n",
    "        The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xf : Pandas dataframe\n",
    "        Dataframe with time shifted variables in each row.\n",
    "    '''\n",
    "    if past == True:\n",
    "        for i in range (1, ndays):\n",
    "            colname = str(variable) + '-{0}'.format(i)\n",
    "            df[colname] = df[variable].shift(+i)\n",
    "    else:\n",
    "        for i in range (1, ndays):\n",
    "            colname = str(variable) + '+{0}'.format(i)\n",
    "            df[colname] = df[variable].shift(-i)\n",
    "    return df\n",
    "\n",
    "class hydrobase():\n",
    "    def __init__(self, station, flowpath, boundarypath):\n",
    "        '''\n",
    "        Initialises a class instance of a streamflow database.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        station : String\n",
    "            Gauging station number, keyed to flow data files.\n",
    "        flowpath : String\n",
    "            File path for the flow data file.\n",
    "        boundarypath : String\n",
    "            File path for the .shp catchment boundary file.  Note that the\n",
    "            .shx file must also be present in the same location.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "        '''\n",
    "        self.station = station\n",
    "        self.flowpath = flowpath\n",
    "        self.flow = pd.read_csv(flowpath)\n",
    "        self.flow = self.flow.drop(self.flow.columns[2], axis=1)\n",
    "        self.flow = self.flow.drop(self.flow.index[0:19])\n",
    "        self.flow.columns = ['Date', 'Flow']\n",
    "        self.flow['Date'] = pd.to_datetime(self.flow['Date'],\n",
    "                                           format='%Y-%m-%d').dt.date\n",
    "        self.flow['Flow'] = self.flow['Flow'].astype(float)\n",
    "        self.boundarypath = boundarypath\n",
    "        self.boundary = gp.read_file(self.boundarypath)\n",
    "        self.points = self.boundary.centroid\n",
    "        self.lat, self.lon = osg.BNG_2_latlon(self.points.x[0],\n",
    "                                              self.points.y[0])\n",
    "        \n",
    "    def meteorological_extraction(self, domain_data):\n",
    "        '''\n",
    "        Extracts meteorological data from the domain datafiles using xarray\n",
    "        and interpolates across the spatial dimensions to obtain a single\n",
    "        time series of the meteorological variables.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        domain_data : Xarray\n",
    "            Array of spatio-temporally distributed variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        local_data : Xarray\n",
    "            Interpolated variables at the centroid coordinates.\n",
    "        '''\n",
    "        local_data = domain_data.interp(longitude=self.lon, latitude=self.lat)\n",
    "        local_data = local_data.to_dataframe().reset_index()\n",
    "        return local_data\n",
    "    \n",
    "    def flow_meteorolgy_combine(self, domain_weather, surface_data, days):\n",
    "        '''\n",
    "        Takes the standard CEH flow file format and combines it with\n",
    "        meteorological data interpolated at the centroid of the catchment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        domain_weather : Xarray opened netcdf file\n",
    "            Meteorological, or other, netcdf files opened using xarray.\n",
    "        surface_data : Xarray opened netcdf file\n",
    "            Soil moisture, or other, surface netcdf files opened using xarray\n",
    "        days : Integer\n",
    "            Number of days needed for antecedent record.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        combined : Pandas dataframe\n",
    "            A single dataframe combining all input meteorological and output\n",
    "            streamflow variables.\n",
    "        '''\n",
    "        weather = self.meteorological_extraction(domain_weather)\n",
    "        surface = self.meteorological_extraction(surface_data)\n",
    "        weather = rename(weather)\n",
    "        surface = rename(surface)\n",
    "        weather['Rain'] = weather['Rain']*1000*24\n",
    "        weather['Date'] = pd.to_datetime(weather['Date'],\n",
    "                                         format='%Y-%m-%d').dt.date\n",
    "        surface['Date'] = pd.to_datetime(surface['Date'],\n",
    "                                         format='%Y-%m-%d').dt.date\n",
    "        weather = weather.drop(['longitude', 'latitude'], axis=1)\n",
    "        surface = surface.drop(['longitude', 'latitude'], axis=1)\n",
    "        weather['Resultant Windspeed'] = (weather['U Windspeed']**2\n",
    "                                          + weather['V Windspeed']**2)**(1/2)\n",
    "        for f in ['Rain','Temperature','Resultant Windspeed','Humidity']:\n",
    "            weather = weather_shift(weather, f, days)\n",
    "            for window in [28, 90, 180]:\n",
    "                ma.stat_roller(weather, f, window, method='mean')\n",
    "        combined = pd.merge(weather, surface, on='Date')\n",
    "        combined = pd.merge(self.flow, combined, how='inner', on='Date')\n",
    "        combined = combined.drop(combined.index[0:179])\n",
    "        return combined\n",
    "    \n",
    "    def output_file(self, domain_weather, surface_data, days):\n",
    "        '''\n",
    "        Applies the meteorological extraction and dataset combination functions\n",
    "        and returns a single combined dataframe which is then exported as a \n",
    "        lumped regression .csv file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        domain_weather : Xarray opened netcdf file\n",
    "            Meteorological, or other, netcdf files opened using xarray.\n",
    "        surface_data : Xarray opened netcdf file\n",
    "            Soil moisture, or other, surface netcdf files opened using xarray\n",
    "        days : Integer\n",
    "            Number of days needed for antecedent record.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "        '''\n",
    "        outdf = self.flow_meteorolgy_combine(domain_weather, surface_data, days)\n",
    "        outpath = str(self.station) + '_lumped.csv'\n",
    "        outdf.to_csv(outpath, index=True)\n",
    "\n",
    "domain_weather = xr.open_mfdataset(['Rainfall.nc',\n",
    "                                    'Pressure.nc'])\n",
    "surface_data = xr.open_dataset('Soil_Moisture.nc')\n",
    "db = pd.read_csv('Catchment_Database.csv')\n",
    "for i in range(len(db)):\n",
    "    cache = hydrobase(db.loc[i][0], db.loc[i][3], db.loc[i][4])\n",
    "    output = cache.output_file(domain_weather, surface_data, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c30e91-00f9-4330-8a24-df676df73079",
   "metadata": {},
   "source": [
    "## Variable Handling\n",
    "The following pair of functions will be used to extract the required columns from the pandas dataframes and then normalise the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44db274e-19f4-41e4-b81d-8576f0cd0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurelocator(df, features):\n",
    "    '''\n",
    "    Identifies the column numbers for features from a dataframe\n",
    "    for use with indexing the array form of that dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Pandas dataframe of all input and output information\n",
    "    features : List\n",
    "        List of target variables or features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_indices : List\n",
    "        List of indices for slicing the converted array.\n",
    "    '''\n",
    "    array_indices = [df.columns.get_loc(f) for f in features]\n",
    "    return array_indices\n",
    "\n",
    "\n",
    "def normalise(df, feature, norm_dict={}, write_cache=True):\n",
    "    '''\n",
    "    Normalises the feature columns of a dataframe, with the option\n",
    "    to retain a cache of the normalisation parameters for use on additional\n",
    "    datasets.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Dataframe containing columns of variables to be normalised\n",
    "    feature : String\n",
    "        Name of feature column to be normalised\n",
    "    norm_dict : Dictionary\n",
    "        Dictionary in which normalisation cache parameters are stored\n",
    "    write_cache : Boolean\n",
    "        Determines whether or not the cache key values will be overwritten\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_indices : List\n",
    "        The input feature column normalised with a dictionary key value entry\n",
    "        made containing all normalisation parameters.\n",
    "    '''\n",
    "    if write_cache == True:\n",
    "        favg = np.mean(df[feature])\n",
    "        fmax = np.max(df[feature])\n",
    "        fmin = np.min(df[feature])\n",
    "        cachelist = [favg, fmax, fmin]\n",
    "        norm_dict[feature] = cachelist\n",
    "        return(df[feature] - favg)/(fmax-fmin)\n",
    "    elif write_cache == False:\n",
    "        try:\n",
    "            cache = norm_dict[feature]\n",
    "            return(df[feature] - cache[0])/(cache[1]-cache[2])\n",
    "        except:\n",
    "            raise KeyError('Normalisation cache specified incorrectly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7689d78-363f-4769-8c4c-26f7a083b662",
   "metadata": {},
   "source": [
    "## Creation of Reflective Weights & Tensor Arrays\n",
    "Using the above functions, we'll: load in the preprocessed data files; partition the data into the training and validation + test sets; create the input and output tensors for the neural network; and normalise the input features using parameters from the training set only.  We'll also create the Reflective Weights needed for both the assessment of our model's performance and for the Reflective loss function, again using information from the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7bd8f-b361-4f33-b7b1-0fd41980e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 53018\n",
    "filename = str(str(station) + '_lumped.csv')\n",
    "rf = pd.read_csv(filename)\n",
    "rf['Date'] = pd.to_datetime(rf['Date'], format='%Y-%m-%d').dt.date\n",
    "\n",
    "### Identify features (with either antecedent proxies or soil moisture levels)\n",
    "days = 13\n",
    "features = ['Rain'] + ['Rain-' + f'{d+1}' for d in range(days)] \\\n",
    "            + ['Temperature'] \\\n",
    "            + ['Temperature-' + f'{d+1}' for d in range(days)] \\\n",
    "            + ['Resultant Windspeed'] \\\n",
    "            + ['Resultant Windspeed-' + f'{d+1}' for d in range(days)] \\\n",
    "            + ['Humidity'] + ['Humidity-' + f'{d+1}' for d in range(days)] \\\n",
    "            + ['Rain_28_Mu','Rain_90_Mu','Rain_180_Mu',\n",
    "               'Temperature_28_Mu','Temperature_90_Mu','Temperature_180_Mu']\n",
    "targets = ['Flow']\n",
    "xspace = featurelocator(rf, features)\n",
    "yspace = featurelocator(rf, targets)\n",
    "\n",
    "###Test/Train data split by years\n",
    "yearlist = [2009+i for i in range(12)]\n",
    "rftrain = rf[~pd.to_datetime(rf['Date']).dt.year.isin(yearlist)]\n",
    "\n",
    "### Fit distribution to training set data & set reflective penalty\n",
    "flows = rftrain['Flow'].values\n",
    "sigma, loc, scale = sp.stats.lognorm.fit(flows)\n",
    "alpha = 1\n",
    "beta = 2\n",
    "kappa = np.exp(-2*(sigma**2)) * (loc*np.exp(2*(sigma**2))+scale)\n",
    "relevance = (1/(sigma*(kappa-loc)/scale*((np.pi*2)**(1/2)))) \\\n",
    "        * np.exp(-(np.log((kappa-loc)/scale)**2)/(2*sigma**2)) / scale*2\n",
    "rf['U_of_y'] = (1/(sigma*(rf['Flow']-loc)/scale*((np.pi*2)**(1/2)))) \\\n",
    "        * np.exp(-(np.log((rf['Flow']-loc)/scale)**2)/(2*sigma**2)) / scale*2\n",
    "rf['Psi'] = RELossWeight(rf['U_of_y'], 1, 1, relevance)\n",
    "rf['Relevance'] = RELossWeight(rf['U_of_y'], 1, 2, relevance)\n",
    "\n",
    "### Normalise features using parameters cached from the training set\n",
    "norm_cache = {}\n",
    "for f in features:\n",
    "    rftrain[f] = ma.normalise(rftrain, f, norm_cache, write_cache=True)\n",
    "    rf[f] = ma.normalise(rf, f, norm_cache, write_cache=False)\n",
    "\n",
    "### Convert dataframe subsets to arrays and then to PyTorch variables\n",
    "rftrain = rf[~pd.to_datetime(rf['Date']).dt.year.isin(yearlist)]\n",
    "trnset = rftrain.to_numpy()\n",
    "X = trnset[:,xspace].reshape(len(trnset), len(xspace)).astype(float)\n",
    "Y = trnset[:,yspace].reshape(len(trnset), len(yspace)).astype(float)\n",
    "G = trnset[:,rf.columns.get_loc('Relevance')].reshape(len(trnset), 1).astype(float)\n",
    "x = torch.from_numpy(X).to(device)\n",
    "y = torch.from_numpy(Y).to(device)\n",
    "g = torch.from_numpy(G).to(device)\n",
    "x, y = Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de1f01-62f3-4e08-84a2-ad5560a59224",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "In this experiment, we're using a relatively simple multilayer perceptron; it will have just 3 layers and will use the swish activiation function.  We'll also randomly initialise the network weights at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ddbe41a-56cd-45b4-8cff-a23e8075d101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mxavier_uniform_(m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     20\u001b[0m         m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m net \u001b[38;5;241m=\u001b[39m AntecedentNET(\u001b[38;5;28mlen\u001b[39m(\u001b[43mxspace\u001b[49m), \u001b[38;5;28mlen\u001b[39m(yspace))\n\u001b[1;32m     23\u001b[0m net\u001b[38;5;241m.\u001b[39mapply(init_weights)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xspace' is not defined"
     ]
    }
   ],
   "source": [
    "class AntecedentNET(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(AntecedentNET, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(16, 1))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = self.linear_layers(z)\n",
    "        return z\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net = AntecedentNET(len(xspace), len(yspace))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7e0ae-6595-416c-a6e6-3ad28e207ae4",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "The optimisation algorithm we use is the Adam algorithm.  As the network is relatively simple, it doesn't require an excessive number of epochs and should train relatively quickly on a modern hardware setup (we used a Macbook with an M2 chip featuring 8 cores and 24 GB of memory and it took a minute or two).  We're using the Reflective Loss function with the alpha and beta hyperparameters set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f2c7c-a286-49fb-b100-8db4ad7a9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.DataParallel(net)\n",
    "net = net.train()\n",
    "net = net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=0.1)\n",
    "loss_list = []\n",
    "for i in range(5000):\n",
    "    y_pred = net(x.float())\n",
    "    loss = torch.abs(torch.mean(RELossFunc(y_pred, y.float(), g.float())))\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_list.append(loss.data)\n",
    "    if(i % 500 == 0):\n",
    "        print('epoch {}, loss {}'.format(i, loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c1b1b-d56e-4f06-8bea-4cf3002b2386",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "With the model trained, we can now examine it's performance on the test set.  The code below will report the metrics evaluated on the training, validation, and test sets but these can be deleted or retained as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8db4b-232a-4112-a180-ed746d795ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()\n",
    "fullset = rf.to_numpy()\n",
    "Z = fullset[:,xspace].reshape(len(fullset), len(xspace)).astype(float)\n",
    "z = torch.from_numpy(Z).to(device)\n",
    "predicted = net(z.float()).data.cpu().numpy()\n",
    "rf['Predicted'] = predicted\n",
    "maxflow = 300\n",
    "rftrain = rf[~pd.to_datetime(rf['Date']).dt.year.isin(yearlist)]\n",
    "rfvalid = rf[pd.to_datetime(rf['Date']).dt.year.isin(yearlist[0:1])]\n",
    "rftest = rf[pd.to_datetime(rf['Date']).dt.year.isin(yearlist[1:])]\n",
    "for df in (rftrain, rfvalid, rftest):\n",
    "    print('- - - - - - - - - - - - - - -')\n",
    "    print('RMSE: ' + str(RMSE(df['Flow'], df['Predicted'])))\n",
    "    print('R\\N{SUPERSCRIPT TWO}: ' + str(R2(df['Flow'], df['Predicted'])))\n",
    "    print('RE: ' + str(RE(df['Flow'], df['Predicted'], df['Psi'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
